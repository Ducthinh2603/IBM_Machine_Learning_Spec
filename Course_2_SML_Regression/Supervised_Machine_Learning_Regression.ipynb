{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d20fa48",
   "metadata": {},
   "source": [
    "# W1\n",
    "## 1. Introduction to Supervised\n",
    "### 1.1. Definition\n",
    "- **Model** (a learning algorithm) to capture larger thing. A good model omits unimportant details while retaining what's important.\n",
    "- Project: Web Search, Fraud Detection, Movie Recommendations, Vehicle Driver Assistance, Web Advertisement, Social Network, Speech Recognition.\n",
    "\n",
    "### 1.2. Types of ML  \n",
    ">- Supervised\n",
    ">- Unsupervised\n",
    ">- Semi-supervised  \n",
    "\n",
    "_Two main modeling approachers:_\n",
    "- Regression: y is numeric.\n",
    "- Classification: y is categorical.\n",
    "\n",
    "### 1.3 Interpretation and Prediction\n",
    "**a. Interpretation**\n",
    "- The primary objective is to train a model to find **insights** from the data.\n",
    "- Workflow: \n",
    "    * Gather x,y; Train model by finding //omega// that give best prediction.\n",
    "    * Focus on //omega// (rather than y_p) to generate insights -> choose **less complex** model, which we can know well, and can understand how feature affect outcome.\n",
    "**b. Prediction**  \n",
    "- The primary objective is to make the best prediction.\n",
    "- Focus on **performance metrics**, which measire the quality of model's prediction.\n",
    "- Without focusing on interpretability, we risk having a Black-box model.\n",
    "\n",
    "## 2. Linear Regression\n",
    "### 2.1. Preprocessing \n",
    "```sh\n",
    "import sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model = model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "```\n",
    "\n",
    "* Making our target variable normally distributed often will lead to better results\n",
    "    If our target is not normally distributed, we can apply a transformation to it and then fit our regression to predict the transformed values.\n",
    "\n",
    "* 2 ways to tell if our target is normally distributed:\n",
    "    * Visually\n",
    "    * Using a statistical test\n",
    "* This is a statistical test that tests whether a distribution is normally distributed or not. It isn't perfect, but suffice it to say: \n",
    "```sh\n",
    "from scipy.stats.mstats import normaltest # D'Agostino K^2 Test\n",
    "normaltest(data.Y.values)\n",
    "```\n",
    "    * This test outputs a \"p-value\". The _higher_ this p-value is the _closer_ the distribution is to normal.\n",
    "    * Frequentist statisticians would say that you accept that the distribution is normal (more specifically: fail to reject the null hypothesis that it is normal) if p > 0.05.\n",
    " \n",
    "### 2.2. BoxCox Transformation \n",
    "The box cox transformation is a parametrized transformation that tries to get distributions \"as close to a normal distribution as possible\".\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$ \\text{boxcox}(y_i) = \\frac{y_i^{\\lambda} - 1}{\\lambda} $$\n",
    "\n",
    "You can think of as a generalization of the square root function: the square root function uses the exponent of 0.5, but box cox lets its exponent vary so it can find the best one.\n",
    "```sh\n",
    "from scipy.stats import boxcox\n",
    "bc_result = boxcox(data.Y)\n",
    "rs = bc_result[0] # transformed data\n",
    "lam = bc_result[1] # lambda\n",
    "```\n",
    "### 2.3. Evaluation Metrics\n",
    "- The variability of the data set can be measured with two sums of squares formulas:\n",
    "The sum of squares of residuals, also called the residual sum of squares:\n",
    "$${\\displaystyle SS_{\\text{res}}=\\sum _{i}(y_{i}-f_{i})^{2}=\\sum _{i}e_{i}^{2}\\,}$$\n",
    "- The total sum of squares (proportional to the variance of the data):\n",
    "$${\\displaystyle SS_{\\text{tot}}=\\sum _{i}(y_{i}-{\\bar {y}})^{2}}$$\n",
    "- The most general definition of the coefficient of determination is:\n",
    "$${\\displaystyle R^{2}=1-{SS_{\\rm {res}} \\over SS_{\\rm {tot}}}}$$\n",
    "- In the best case, the modeled values exactly match the observed values, which results in ${\\displaystyle SS_{\\text{res}}=0}$ and ${\\displaystyle R^{2}=1}$. A baseline model, which always predicts ${\\displaystyle {\\bar {y}}}$, will have ${\\displaystyle R^{2}=0}$. Models that have worse predictions than this baseline will have a negative ${\\displaystyle R^{2}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d9e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
